# Importing the required libraries
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.utils import to_categorical

# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Preprocess the data
x_train = x_train / 255.0
x_test = x_test / 255.0

# One-hot encoding the target labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# Build the neural network model
model = Sequential()
model.add(Flatten(input_shape=(28, 28)))   # Flatten the 28x28 images into a 1D array
model.add(Dense(128, activation='relu'))    # Hidden layer with 128 neurons and ReLU activation
model.add(Dense(64, activation='relu'))     # Hidden layer with 64 neurons and ReLU activation
model.add(Dense(10, activation='softmax'))  # Output layer with 10 neurons and softmax activation

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss, accuracy = model.evaluate(x_test, y_test)
print("Test accuracy:", accuracy)

# Print some of the handwritten digits for prediction
for i in range(5):
    image = x_test[i].reshape(1, 28, 28)
    predicted_probs = model.predict(image)[0]
    predicted_class = np.argmax(predicted_probs)
    print("This number is:", predicted_class)

    plt.imshow(x_test[i], cmap='gray')
    plt.show()
